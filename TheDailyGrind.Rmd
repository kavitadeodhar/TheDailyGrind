---
title: "The Daily Grind"
author: "Sumit Gupta, Kavita Deodhar"
date: "April 18, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment = NA, echo=FALSE, message = FALSE, warnings = FALSE)
```

##Overview
Social Media is a daily diary for many people. It is now easy to let your voice be heard even if it is about a simple thing like our regular cup of coffee. There are many options and people tend to be loyal to their brands. The intent is to explore which brand do people favor based on their tweets. We would like to show how data could provide highly useful insights about the business. By harnessing the power of data companies can provide better service to the customers. It could also be used to see how a company could expand their presence. Also the intent is to explore whether momentary emotion conveyed via a tweet has any correlation on customer satisfaction index. We will compare CSAT data from ACSI(American customer satisfaction index) to the scores generated using our code. 

For the purpose of this project we are going to use the following brands.

<img src="https://raw.githubusercontent.com/kavitadeodhar/TheDailyGrind/master/dunkin.png">
<img src="https://raw.githubusercontent.com/kavitadeodhar/TheDailyGrind/master/Starbucks_Coffee_Logo.svg.png">
<img src="https://raw.githubusercontent.com/kavitadeodhar/TheDailyGrind/master/mcdonalds-logo.jpg">
<img src="https://raw.githubusercontent.com/kavitadeodhar/TheDailyGrind/master/panera-bread-logo.png">
<img src="https://raw.githubusercontent.com/kavitadeodhar/TheDailyGrind/master/subway.jpg">

##Initial Questions
Here's what would like to learn and accomplish:
1. Perform sentiment analysis on twitter data for various coffee shop brands and determine which brand is most favored.
2. Does brand preference change in different parts of the country?
3. Perform regression analysis and determine if emotion expressed on twitter ties into/affects/compares to customer satisfaction index calculated using multivariate customer experience data.
4. We would like to see if there is a trend (plot against time) but we may not be able to do it due to the size of data.
5.Check if the ACSI index values are close to the tweet scores.

## Data and Exploratory Analysis
Let us start analysis by gathering twitter data for the above brands. We will be using Twitter API for this purpose.

Considering the fact that most people may exclusively communicate using emojis, we decided not to drop emojis out of our analysis.Source of this data is "https://raw.githubusercontent.com/today-is-a-good-day/Emoticons/master/emDict.csv"

In order to extract geocode information from tweets, we used following cities and surrounding areas within a 200 mile radius within our search criteria : DC,New York,San Fransisco,Colorado,Mountainview,Tampa,Austin,Boston,Seatle,Vegas,Atlanta,Raleigh,Chicago,Los Angeles,Dallas. 

```{r, echo=FALSE,message=FALSE,warning=FALSE}
library(twitteR)
library(dplyr)
library(stringr)
library(tm)
library(DataCombine)
library(wordcloud)
library(RColorBrewer)
library(readr)
library(syuzhet)
library(ggplot2)
library(grid)
library(maps)

##### Twitter authentication 

api_key <- "Od5rpMWWch3FVBDVBfeFVUjV3"
api_secret <- "zibZTX8dUmtSEdsHtIG20ErRhnAjA3GqFC2pGSZJ54ImG1Y6B6"
access_token <- "915794173-7YxpWXQRfsjbmIWFuSJbr5EzPLJqQ9XSdOjkTmO0"
access_token_secret <- "P09dahyCxPhuTpJ3sXhZfNgZEALpr8jkaWIA2DwEcMbLq"
# This option is to force direct authentication.
options(httr_oauth_cache = TRUE)
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
##################################################################
```


First we create data sets for each of the individual brands and replace the emojis.

```{r,message=FALSE,warning=FALSE}
#Read the emoji decoder file. This file will be used to lookup the sentiments that each emoticon represents.
emoji_decoder <- read_delim("https://raw.githubusercontent.com/today-is-a-good-day/Emoticons/master/emDict.csv",delim = ";")

#Following cities are used to pull the tweets for sentiment analysis: DC,New York,San Fransisco,Colorado,Mountainview,Tampa,Austin,Boston,Seatle,Vegas,Atlanta,Raleigh,Chicago,Los Angeles,Dallas

N=500  # Number tweets to request from each query
S=200  # radius in miles from each city
#Below is the latitude and longitude information for the cities mentioned above.
lats=c(32.3,33.5,34.7,37.2,41.2,46.8,46.6,37.2,43,42.7,40.8,36.2,38.6,35.8,40.3,43.6,40.8,44.9,44.9) 
lons=c(-86.3,-112,-92.3,-84.4,-93.3,-104.8,-100.8,-112,-93.3,-89,-84.5,-111.8,-86.8,-92.2,-78.6,-76.8,-116.2,-98.7,-123,-93)

#Dunkin Data Set Creation
#Twitter search for Dunkin
Dunkin <- do.call(rbind,lapply(1:length(lats), function(i) searchTwitter('Dunkin',
              lang="en",n=N,resultType="recent",
              
              geocode=paste(lats[i],lons[i],paste0(S,"mi"),sep=","))))

#Converting the data to data frame
Dunkin_df <- twListToDF(Dunkin)


#Adding a column 
Dunkin_geo_df <- mutate(Dunkin_df,QSR="Dunkin")

#Convert the emoji data to byte
Dunkin_byte <- data.frame(text_emoji=iconv(Dunkin_df$text,"latin1","ASCII","byte")) 

#Join the datasets
Dunkin_bind <- cbind(Dunkin_df,Dunkin_byte) %>% select (text_emoji,created,id)

#Convert the emoji data to english words
Dunkin_convert_emoji <- FindReplace(data = Dunkin_bind,Var = "text_emoji",replaceData = emoji_decoder,from = "R-encoding",to="Description",exact = FALSE)
colnames(Dunkin_convert_emoji)[1] <- "text"

#McDonalds Data Set Creation
#Twitter search for McDonalds
McDonalds <- do.call(rbind,lapply(1:length(lats), function(i) searchTwitter('McDonalds',
              lang="en",n=N,resultType="recent",
              geocode=paste(lats[i],lons[i],paste0(S,"mi"),sep=","))))

#Converting the data to data frame
McDonalds_df <- twListToDF(McDonalds)

#Adding a column 
McDonalds_geo_df <- mutate(McDonalds_df,QSR="McDonalds")

#Convert the emoji data to byte
McDonalds_byte <- data.frame(text_emoji=iconv(McDonalds_df$text,"latin1","ASCII","byte")) 

#Join the datasets
McDonalds_bind <- cbind(McDonalds_df,McDonalds_byte) %>% select (text_emoji,created,id)

#Convert the emoji data to english words
McDonalds_convert_emoji <- FindReplace(data = McDonalds_bind,Var = "text_emoji",replaceData = emoji_decoder,from = "R-encoding",to="Description",exact = FALSE)
colnames(McDonalds_convert_emoji)[1] <- "text"

#Starbucks Data Set Creation
#Twitter search for Starbucks
Starbucks <- do.call(rbind,lapply(1:length(lats), function(i) searchTwitter('Starbucks',
              lang="en",n=N,resultType="recent",
              geocode=paste(lats[i],lons[i],paste0(S,"mi"),sep=","))))

#Converting the data to data frame
Starbucks_df <- twListToDF(Starbucks)

#Adding a column 
Starbucks_geo_df <- mutate(Starbucks_df,QSR="Starbucks")

#Convert the emoji data to byte
Starbucks_byte <- data.frame(text_emoji=iconv(Starbucks_df$text,"latin1","ASCII","byte")) 

#Join the datasets
Starbucks_bind <- cbind(Starbucks_df,Starbucks_byte) %>% select (text_emoji,created,id)

#Convert the emoji data to english words
Starbucks_convert_emoji <- FindReplace(data = Starbucks_bind,Var = "text_emoji",replaceData = emoji_decoder,from = "R-encoding",to="Description",exact = FALSE)
colnames(Starbucks_convert_emoji)[1] <- "text"


#Panera Data Set Creation
#Twitter search for Panera
Panera <- do.call(rbind,lapply(1:length(lats), function(i) searchTwitter('Panera',
              lang="en",n=N,resultType="recent",
              geocode=paste(lats[i],lons[i],paste0(S,"mi"),sep=","))))

#Converting the data to data frame
Panera_df <- twListToDF(Panera)

#Adding a column 
Panera_geo_df <- mutate(Panera_df,QSR="Panera")

#Convert the emoji data to byte
Panera_byte <- data.frame(text_emoji=iconv(Panera_df$text,"latin1","ASCII","byte")) 

#Join the datasets
Panera_bind <- cbind(Panera_df,Panera_byte) %>% select (text_emoji,created,id)

#Convert the emoji data to english words
Panera_convert_emoji <- FindReplace(data = Panera_bind,Var = "text_emoji",replaceData = emoji_decoder,from = "R-encoding",to="Description",exact = FALSE)
colnames(Panera_convert_emoji)[1] <- "text"

#Subway Data Set Creation
#Twitter search for Subway
Subway <- do.call(rbind,lapply(1:length(lats), function(i) searchTwitter('@Subway',
              lang="en",n=N,resultType="recent",
              geocode=paste(lats[i],lons[i],paste0(S,"mi"),sep=","))))

#Converting the data to data frame
Subway_df <- twListToDF(Subway)

#Adding a column 
Subway_geo_df <- mutate(Subway_df,QSR="Subway")

#Convert the emoji data to byte
Subway_byte <- data.frame(text_emoji=iconv(Subway_df$text,"latin1","ASCII","byte")) 

#Join the datasets
Subway_bind <- cbind(Subway_df,Subway_byte) %>% select (text_emoji,created,id)

#Convert the emoji data to english words
Subway_convert_emoji <- FindReplace(data = Subway_bind,Var = "text_emoji",replaceData = emoji_decoder,from = "R-encoding",to="Description",exact = FALSE)
colnames(Subway_convert_emoji)[1] <- "text"

#Creating data set to plot it on USA map. Union of all the different restuarant data
QSR_geo_df <- rbind(Dunkin_geo_df,Starbucks_geo_df,McDonalds_geo_df,Panera_geo_df,Subway_geo_df)

#rm(list=ls())
```
Note that the number of tweets retrieved for all brands is the same.


## Plotting data on US map

As the first part of our analysis,we are going to map all the tweets based on the geo code on US map. This is an attempt to analyze brand preferance per region.


```{r, message=FALSE,warning=FALSE}
#Plotting the QSR data based on latitude and longitude 
map.data <- map_data("state")
points <- data.frame(x = as.numeric(QSR_geo_df$longitude), y = as.numeric(QSR_geo_df$latitude),QSR=QSR_geo_df$QSR)
points <- points[points$y > 25, ]
p <- ggplot(map.data) + geom_map(aes(map_id = region), map = map.data, fill = "#f1f1f1", 
    color = "#996600", size = 0.25) + expand_limits(x = map.data$long, y = map.data$lat) + 
    theme(axis.line = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), 
        axis.title = element_blank(), panel.background = element_blank(), panel.border = element_blank(), 
        panel.grid.major = element_blank(), plot.background = element_blank(),
        plot.margin = unit(0 * c(-1.5, -1.5, -1.5, -1.5), "lines")) + geom_point(data = points, 
    aes(x = x, y = y,color=QSR), size = 2)
p+ scale_color_manual(values=c("#F21F88", "#9748a8","#ECC56D","#00592D","#fef035"))
```

From the map above,we can see the distribution of tweets across the country. We see that the number of tweets for Panera look higher. However th number of tweets for every brand was the same.This simply means that the latitude longitude information was available for larger number of tweets for Panera than the other brands.

Note: We have included in the website, a couple of extra map screenshots obtained by running the code at various times of the day. We can see that Dunkin and Starbucks are popular during the morning or breakfast hours and preference for Panera strongly increases during the later part or lunch hours of the day.

## Cleaning the data
####Using the tm package to clean the tweets.


Next we proceed with cleaning the data before calculating the score for every tweet.This includes removing special characters, punctuations, words like http, https.


```{r, message=FALSE,warning=FALSE}
#Function to clean the tweet data
clean_tweets <- function(datafeed) {
#Remove handles
datafeed_list <- str_replace_all(datafeed$text, "@\\w+", "")
datafeed_list <- Corpus(VectorSource(datafeed_list))
#Remove punctuation
datafeed_list <- tm_map(datafeed_list,removePunctuation)
#Remove Stopwords
datafeed_list <- tm_map(datafeed_list, removeWords, stopwords("english"))
#Convert the text to lower case
datafeed_list <- tm_map(datafeed_list,content_transformer(tolower))
datafeed_list <- tm_map(datafeed_list,removeWords,c("amp","http","https"))
#Remove whitespace
datafeed_list <- tm_map(datafeed_list,stripWhitespace)
}

#Calling clean_tweets function to clean up the data
Dunkin_vector <- clean_tweets(Dunkin_convert_emoji)
Starbucks_vector <- clean_tweets(Starbucks_convert_emoji)
McDonalds_vector <- clean_tweets(McDonalds_convert_emoji)
Subway_vector <- clean_tweets(Subway_convert_emoji)
Panera_vector <- clean_tweets(Panera_convert_emoji)

#Convert Dunkin vector to dataframe
Dunkin_clean_df <- data.frame(text=unlist(sapply(Dunkin_vector, `[`,"content")),stringAsFactors=F) 
Dunkin_clean_df <- cbind(Dunkin_clean_df$text,Dunkin_convert_emoji) %>% select (-text)  
colnames(Dunkin_clean_df)[1] <- "text"

#Convert Starbucks vector to dataframe
Starbucks_clean_df <- data.frame(text=unlist(sapply(Starbucks_vector, `[`,"content")),stringAsFactors=F)
Starbucks_clean_df <- cbind(Starbucks_clean_df$text,Starbucks_convert_emoji) %>% select (-text)  
colnames(Starbucks_clean_df)[1] <- "text"

#Convert McDonalds vector to dataframe
McDonalds_clean_df <- data.frame(text=unlist(sapply(McDonalds_vector, `[`,"content")),stringAsFactors=F) 
McDonalds_clean_df <- cbind(McDonalds_clean_df$text,McDonalds_convert_emoji) %>% select (-text)  
colnames(McDonalds_clean_df)[1] <- "text"

#Convert Panera vector to dataframe
Panera_clean_df <- data.frame(text=unlist(sapply(Panera_vector, `[`,"content")),stringAsFactors=F) 
Panera_clean_df <- cbind(Panera_clean_df$text,Panera_convert_emoji) %>% select (-text)  
colnames(Panera_clean_df)[1] <- "text"

#Convert Subway vector to dataframe
Subway_clean_df <- data.frame(text=unlist(sapply(Subway_vector, `[`,"content")),stringAsFactors=F) 
Subway_clean_df <- cbind(Subway_clean_df$text,Subway_convert_emoji) %>% select (-text)  
colnames(Subway_clean_df)[1] <- "text"
```
## Calculating tweet scores for all brands.

#### We will use the nrc sentiment categories.The get_nrc_sentiment implements Saif Mohammad's NRC Emotion lexicon. According to Mohammad, "the NRC emotion lexicon is a list of words and their associations with eight emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive)" (See http://www.purl.org/net/NRCemotionlexicon). The get_nrc_sentiment function returns a data frame in which each row represents a sentence from the original file. Source of the above information is "https://cran.r-project.org/web/packages/syuzhet/vignettes/syuzhet-vignette.html"

```{r, message=FALSE,warning=FALSE}
######################
##This is the function that allocates scores to tweets and groups them by emotion category.
get_sentiment <- function(datafeed) {
sentiment <- get_nrc_sentiment(as.character(datafeed$text))
head(sentiment)
qsr_tweets <- cbind(datafeed, sentiment)
sentimentTotals <- data.frame(colSums(qsr_tweets[,c(4:11)]))
names(sentimentTotals) <- "count"
sentimentTotals <- cbind("sentiment" = rownames(sentimentTotals), sentimentTotals)
}

#get sentiment for Starbucks
Starbucks_sentimentTotals <- get_sentiment(Starbucks_clean_df)
rownames(Starbucks_sentimentTotals) <- NULL
Starbucks_sentimentTotals <- mutate(Starbucks_sentimentTotals,QSR = "Starbucks")

#get sentiment for Dunkin
Dunkin_sentimentTotals <- get_sentiment(Dunkin_clean_df)
rownames(Dunkin_sentimentTotals) <- NULL
Dunkin_sentimentTotals <- mutate(Dunkin_sentimentTotals,QSR = "Dunkin")

#get sentiment for McDonalds
McDonalds_sentimentTotals <- get_sentiment(McDonalds_clean_df)
rownames(McDonalds_sentimentTotals) <- NULL
McDonalds_sentimentTotals <- mutate(McDonalds_sentimentTotals,QSR = "McDonalds")

#get sentiment for Panera
Panera_sentimentTotals <- get_sentiment(Panera_clean_df)
rownames(Panera_sentimentTotals) <- NULL
Panera_sentimentTotals <- mutate(Panera_sentimentTotals,QSR = "Panera")

#get sentiment for Subway
Subway_sentimentTotals <- get_sentiment(Subway_clean_df)
rownames(Subway_sentimentTotals) <- NULL
Subway_sentimentTotals <- mutate(Subway_sentimentTotals,QSR = "Subway")

#combine the dataset to compare the brands.
sentimentTotals <- rbind(Dunkin_sentimentTotals,Starbucks_sentimentTotals,McDonalds_sentimentTotals,Panera_sentimentTotals,Subway_sentimentTotals) %>% arrange(QSR,sentiment) 
```
##Sentiment per brand
####Below is the plot of number of tweets per brand per category.It helps is understand the general emotion for each brand and also per category for every brand.

```{r, message=FALSE,warning=FALSE}
#Plot of data for sentiment groups comparing all brands using the brands colors.

ggplot(data = sentimentTotals, aes(x = sentiment, y = count)) +       geom_bar(aes(fill = QSR), stat = "identity" , position = position_dodge()) +scale_fill_manual(values = c("#F21F88", "#9748a8","#ECC56D","#00592D","#fef035")) + xlab("Sentiment") + ylab("Total Count") + ggtitle("Total Sentiment Score for All Tweets")

```
##Dunkin seems to be the brand people trust the most and Starbucks gives most joy to people.There also is a lot of negative emotion for McDonalds.Tweets about Subway and Panera seem to be spread equally over all categories of emotion.

##The categories of sentiments are fine, but what are those words that people associate with these brands? What is it that people are talking about in regards to the brands?

##Word cloud helps us do just that. Below are the word clouds for all the 5 of our brands. Note that the font is largest for the most popular word and decreases as the frequency decreases.

#########################
```{r, echo=FALSE,message=FALSE,warning=FALSE}
#Assign title to wordcloud
title_wordcloud <- function(QSR_name,hexcolorname){
layout(matrix(c(1,2),nrow=2),heights = c(1,4))
par(mar=rep(0,4))
plot.new()
text(x=0.5,y=0.5,QSR_name,col = hexcolorname, cex = 2.0) 
}


#Word cloud for Dunkin

title_wordcloud("Dunkin' Donuts","#F21F88")
pal <- brewer.pal(9,"RdPu")
pal <- pal[-(1:4)]
set.seed(123)
wordcloud(words = Dunkin_vector, scale=c(5,0.5), max.words=250, random.order=FALSE, rot.per=0.45, use.r.layout=FALSE, colors=pal)

#Word cloud for Starbucks

title_wordcloud("Starbucks","#00592D")
pal <- brewer.pal(9,"YlGn")
pal <- pal[-(1:4)]
set.seed(123)
wordcloud(words = Starbucks_vector, scale=c(5,0.5), max.words=250, random.order=FALSE, rot.per=0.45, use.r.layout=FALSE, colors=pal)

#Word cloud for McDonalds

title_wordcloud("McDonald's","#9748a8")
pal <- brewer.pal(9,"YlOrRd")
pal <- pal[-(1:4)]
set.seed(123)
wordcloud(words = McDonalds_vector, scale=c(5,0.5), max.words=250, random.order=FALSE, rot.per=0.45, use.r.layout=FALSE, colors=pal)

#Word cloud for Panera

title_wordcloud("Panera Bread","#ECC56D")
pal <- brewer.pal(7,"RdYlGn")
pal <- pal[-(1:4)]
set.seed(123)
wordcloud(words = Panera_vector, scale=c(5,0.5), max.words=250, random.order=FALSE, rot.per=0.45, use.r.layout=FALSE, colors=pal)

#Word Cloud for Subway

title_wordcloud("Subway","#fef035")
pal <- brewer.pal(9,"Greens")
pal <- pal[-(1:4)]
set.seed(123)
wordcloud(words = Subway_vector, scale=c(5,0.5), max.words=250, random.order=FALSE, rot.per=0.45, use.r.layout=FALSE, colors=pal)
```
####The most used words will change through out the course of the day as more tweets are added from the new day and old ones removed.

####We have been able to gather tweets only for a ceratin number of days due to restrictions on the Twitter API. So we cannot see the trend of sentiment over years or months. However we will calculate and plot negative and positive sentiment over the last few days for all the brands.

```{r, message=FALSE,warning=FALSE}

####Reading ACSI data for Dunkin and Starbucks. Intent is to check if there is any co relation between the sentiment and satisfaction index.

library(XML)
#library(tidyr)

get_tweet_score <- function(datafeed){
sentiment <- get_nrc_sentiment(as.character(datafeed$text))
head(sentiment)
qsr_tweets <- cbind(datafeed, sentiment)
}

library(reshape2)
#This function is used to create sentiment over time plot

sentiment_time <- function (pass_df,graph_title,col1,col2)
{
Hist <- get_tweet_score(pass_df)

posnegtime <- Hist %>% 
        group_by(created = cut(created, breaks="1 day")) %>%
        summarise(negative = mean(negative),
                  positive = mean(positive)) %>% melt

names(posnegtime) <- c("timestamp", "sentiment", "meanvalue")
posnegtime$sentiment = factor(posnegtime$sentiment,levels(posnegtime$sentiment)[c(2,1)])

ggplot(data = posnegtime, aes(x = timestamp, y = meanvalue, group = sentiment)) +
        geom_line(size = 2.5, alpha = 0.7, aes(color = sentiment)) + theme(axis.text.x = element_text(angle = 40,size=8,vjust=0.3))+
        ylab("Average sentiment score") + ggtitle(graph_title) +scale_color_manual(values=c(col1,col2))
}

#Calling the sentiment_time function for each of the QSR's to plot sentiment over time
sentiment_time(Dunkin_clean_df,"Dunkin Sentiment Over Time","#F21F88","#ff6600")
sentiment_time(Starbucks_clean_df,"Starbucks Sentiment Over Time","#00704a","#222222")
sentiment_time(McDonalds_clean_df,"McDonalds Sentiment Over Time","#9748a8","#bf0c0c")
sentiment_time(Panera_clean_df,"Panera Sentiment Over Time","#ECC56D","#00592D")
sentiment_time(Subway_clean_df,"Subway Sentiment Over Time","#fef035","#00592D")
```

##Comparing the distribution of scores.
####Next step would be to check the distribution of scores or sentiments. The difference between the positive and negative scores will give the general tweet sentiment or score. negative value will indicate negative sentiment and positive value will contain positive sentiment.  

```{r, message=FALSE,warning=FALSE}
#Function to Plot the score for different QSR. This function takes the restaurant name, color and plot description as input. The difference between the positive and negative scores gives the "total" score of the tweet.
hist_time <- function (pass_df,colour,QSR_name)
{
Hist <- get_tweet_score(pass_df)
hist(Hist$positive - Hist$negative,col = colour,xlab = "Sentiment Score",main = QSR_name, right = FALSE)
}

#Calling the above functions and plotting the scores for 5 QSR's
hist_time(Dunkin_clean_df,"#F21F88","Dunkin Score Histogram")
hist_time(Starbucks_clean_df,"#00592D","Startbucks Score Histogram")
hist_time(McDonalds_clean_df,"#9748a8","McDonalds Score Histogram")
hist_time(Panera_clean_df,"#ECC56D","Panera Score Histogram")
hist_time(Subway_clean_df,"#fef035","Subway Score Histogram")
```

##Compare with ACSI data.
####The customer satisfaction (ACSI) index score is calculated as a weighted average of three survey questions that measure different facets of satisfaction with a product or service. ACSI researchers use proprietary software technology to estimate the weighting for each question.Source of this information is "http://www.theacsi.org"

####The last step is to compare the average tweet score(calculated for tweets in our dataset which are over a period of few days only) for all our brands with the ACSI index score which is determined as described above.

```{r, message=FALSE,warning=FALSE}
#Getting the acsi data for the QSR's. 

acsi_url <- "http://theacsi.org/index.php?option=com_content&view=article&id=147&catid=&Itemid=212&i=Limited-Service+Restaurants"
acsi_df <- readHTMLTable(acsi_url,header=T,stringsAsFactors=F,which = 1)
colnames(acsi_df)[1] <-"QSR"
acsi_df <- filter(acsi_df,QSR %in% c("Dunkin' Donuts","Starbucks","Panera Bread","McDonald's","Subway"))
acsi_df[2,1] <- "Dunkin"
acsi_df[1,1] <- "Panera"
acsi_df[5,1] <- "McDonalds"
library(tidyr)
acsi_df <- acsi_df%>%gather(key = Year,value = Csat_pc,20:23)
acsi_df <- acsi_df %>% select(QSR,Year,Csat_pc) %>% filter(Csat_pc != 'NM')
acsi_df <- acsi_df %>% group_by(QSR)%>% summarise(average_csat = mean(as.numeric(Csat_pc)))

#Calculate the Dunkin score from the tweets dataframe and calcualte the positive percent using following formula. (total positive score*100)/(totol negative score + total positive score)

Dunkin_Tweet_score <-get_tweet_score(Dunkin_clean_df)
Dunkin_Tweet_score<- Dunkin_Tweet_score%>%summarize(total_pos_score = sum(positive),total_neg_score = sum(negative)) %>% mutate(QSR ="Dunkin")
Dunkin_Tweet_score <- mutate(Dunkin_Tweet_score,total_score=total_pos_score+total_neg_score, total_pos_pc = total_pos_score*100/(total_pos_score+total_neg_score))

#Calculate the Starbucks score from the tweets dataframe and calcualte the positive percent using following formula. (total positive score*100)/(totol negative score + total positive score)

Starbucks_Tweet_score <-get_tweet_score(Starbucks_clean_df)
Starbucks_Tweet_score<- Starbucks_Tweet_score%>%summarize(total_pos_score = sum(positive),total_neg_score = sum(negative)) %>% mutate(QSR ="Starbucks")
Starbucks_Tweet_score <- mutate(Starbucks_Tweet_score,total_score=total_pos_score+total_neg_score, total_pos_pc = total_pos_score*100/(total_pos_score+total_neg_score))

#Calculate the McDonalds score from the tweets dataframe and calcualte the positive percent using following formula. (total positive score*100)/(totol negative score + total positive score)

McDonalds_Tweet_score <-get_tweet_score(McDonalds_clean_df)
McDonalds_Tweet_score<- McDonalds_Tweet_score%>%summarize(total_pos_score = sum(positive),total_neg_score = sum(negative)) %>% mutate(QSR ="McDonalds")
McDonalds_Tweet_score <- mutate(McDonalds_Tweet_score,total_score=total_pos_score+total_neg_score, total_pos_pc = total_pos_score*100/(total_pos_score+total_neg_score))

#Calculate the Panera score from the tweets dataframe and calcualte the positive percent using following formula. (total positive score*100)/(totol negative score + total positive score)

Panera_Tweet_score <-get_tweet_score(Panera_clean_df)
Panera_Tweet_score<- Panera_Tweet_score%>%summarize(total_pos_score = sum(positive),total_neg_score = sum(negative)) %>% mutate(QSR ="Panera")
Panera_Tweet_score <- mutate(Panera_Tweet_score,total_score=total_pos_score+total_neg_score, total_pos_pc = total_pos_score*100/(total_pos_score+total_neg_score))

#Calculate the Subway score from the tweets dataframe and calcualte the positive percent using following formula. (total positive score*100)/(totol negative score + total positive score)

Subway_Tweet_score <-get_tweet_score(Subway_clean_df)
Subway_Tweet_score<- Subway_Tweet_score%>%summarize(total_pos_score = sum(positive),total_neg_score = sum(negative)) %>% mutate(QSR ="Subway")
Subway_Tweet_score <- mutate(Subway_Tweet_score,total_score=total_pos_score+total_neg_score, total_pos_pc = total_pos_score*100/(total_pos_score+total_neg_score))

Tweet_Score <- rbind(Starbucks_Tweet_score,Dunkin_Tweet_score,McDonalds_Tweet_score,Panera_Tweet_score,Subway_Tweet_score)

Combined_scores <- inner_join(Tweet_Score,acsi_df,by = "QSR")

Combined_scores_lm <- select(Combined_scores,QSR,total_pos_pc,average_csat)

tbl_df(Combined_scores_lm)

#Plotting the linear model for following data. ACSI score versus Calcualted scores

ggplot(Combined_scores_lm, aes(x=total_pos_pc, y=average_csat) ) +
geom_point( aes(color=QSR), size=5 ) + geom_smooth( method="lm", se=FALSE)

```

####As can be seen from the table with average tweet scores calculated by us and ACSI index determined through surveys and research are close. There is a good possibility that the tweet scores could be used to predict the satisfaction index.

##Conclusion:
####We were able 

